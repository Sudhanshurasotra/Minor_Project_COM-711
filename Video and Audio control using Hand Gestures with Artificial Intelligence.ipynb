{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ae0ac-5f11-4e31-a96a-97f6e86f20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1fa2d-47c1-4fa4-a192-01243d75aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hand detector and classifier\n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier('Model/keras_model.h5', 'Model/labels.txt')\n",
    "\n",
    "# Set video parameters\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "labels = [\"free_hand\", \"right\", \"left\", \"v_up\", \"v_down\", \"max\", \"min\", \"stop\"]\n",
    "\n",
    "# Open video file\n",
    "video_path = \"uploaded_video.mp4\"  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Video control variables\n",
    "control_delay = 1  # Seconds between actions\n",
    "last_action_time = time.time() - control_delay\n",
    "paused = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    if not paused:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print(\"End of video or error.\")\n",
    "            break\n",
    "    else:\n",
    "        success, _ = cap.read()  # Read the frame but don't advance\n",
    "\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "        imgCrop = img[y - offset: y + h + offset, x - offset: x + w + offset]\n",
    "\n",
    "        if imgCrop.size == 0:\n",
    "            print(\"Empty image crop. Skipping frame.\")\n",
    "            continue\n",
    "\n",
    "        aspectRatio = h / w\n",
    "        if aspectRatio > 1:\n",
    "            k = imgSize / h\n",
    "            wCal = math.ceil(k * w)\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            k = imgSize / w\n",
    "            hCal = math.ceil(k * h)\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "        prediction, index = classifier.getPrediction(imgWhite, draw=True)\n",
    "        label = labels[index]\n",
    "\n",
    "        # Perform actions based on detected label\n",
    "        current_time = time.time()\n",
    "        if label == \"v_up\" and current_time - last_action_time >= control_delay:\n",
    "            print(\"Volume up\")\n",
    "            last_action_time = current_time\n",
    "        elif label == \"v_down\" and current_time - last_action_time >= control_delay:\n",
    "            print(\"Volume down\")\n",
    "            last_action_time = current_time\n",
    "        elif label == \"stop\" and current_time - last_action_time >= control_delay:\n",
    "            paused = not paused\n",
    "            print(\"Pause/Play toggled\")\n",
    "            last_action_time = current_time\n",
    "        elif label == \"max\" and current_time - last_action_time >= control_delay:\n",
    "            print(\"Full screen\")\n",
    "            last_action_time = current_time\n",
    "        elif label == \"min\" and current_time - last_action_time >= control_delay:\n",
    "            print(\"Exit full screen\")\n",
    "            last_action_time = current_time\n",
    "        elif label == \"right\" and current_time - last_action_time >= control_delay:\n",
    "            current_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame + 30)  # Skip 30 frames\n",
    "            print(\"Skipped forward\")\n",
    "            last_action_time = current_time\n",
    "        elif label == \"left\" and current_time - last_action_time >= control_delay:\n",
    "            current_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, current_frame - 30))  # Go back 30 frames\n",
    "            print(\"Skipped backward\")\n",
    "            last_action_time = current_time\n",
    "\n",
    "    # Display the video frame\n",
    "    cv2.imshow(\"Video\", imgOutput)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
